{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.decomposition import PCA\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def read_csv(csv_path):\n",
    "    np_path_XYs = np.genfromtxt(csv_path, delimiter=',')\n",
    "    path_XYs = []\n",
    "    for i in np.unique(np_path_XYs[:, 0]):\n",
    "        npXYs = np_path_XYs[np_path_XYs[:, 0] == i][:, 1:]\n",
    "        XYs = []\n",
    "        for j in np.unique(npXYs[:, 0]):\n",
    "            XY = npXYs[npXYs[:, 0] == j][:, 1:]\n",
    "            if XY.shape[0] > 1:  # Ensure at least two points for interpolation\n",
    "                XYs.append(XY)\n",
    "        path_XYs.append(XYs)\n",
    "    return path_XYs\n",
    "\n",
    "def data_cleaning(points):\n",
    "    \"\"\"Ensure there are no NaN or infinite values.\"\"\"\n",
    "    points = np.nan_to_num(points, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return points\n",
    "\n",
    "def uniform_sampling(points, num_samples):\n",
    "    if len(points) < 2:\n",
    "        return points  # Not enough points to interpolate\n",
    "    cumulative_dist = np.cumsum(np.sqrt(np.sum(np.diff(points, axis=0)**2, axis=1)))\n",
    "    cumulative_dist = np.insert(cumulative_dist, 0, 0)  # Add the starting point\n",
    "    try:\n",
    "        distance_fn = interp1d(cumulative_dist, points, axis=0, kind='linear')\n",
    "        uniform_dist = np.linspace(0, cumulative_dist[-1], num_samples)\n",
    "        return distance_fn(uniform_dist)\n",
    "    except ValueError as e:\n",
    "        print(f\"Interpolation error: {e}\")\n",
    "        return points\n",
    "    \n",
    "uniform_points = 150\n",
    "\n",
    "def normalisation(points, num_points=uniform_points):\n",
    "    if len(points) == 0:\n",
    "        return np.zeros((num_points, 2))\n",
    "    points = points - np.mean(points, axis=0)\n",
    "    scale = np.sqrt(np.sum(points**2, axis=1)).max()\n",
    "    if scale == 0:\n",
    "        return np.zeros((num_points, 2))\n",
    "    points /= scale\n",
    "    points = data_cleaning(points)\n",
    "    return uniform_sampling(points, num_points)\n",
    "\n",
    "def hungarian_matching(points, reference_points):\n",
    "    cost_matrix = np.linalg.norm(points[:, np.newaxis] - reference_points, axis=2)\n",
    "    cost_matrix = data_cleaning(cost_matrix)\n",
    "    if np.any(np.isnan(cost_matrix)) or np.any(np.isinf(cost_matrix)):\n",
    "        raise ValueError(\"Cost matrix contains NaN or infinite values.\")\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return cost_matrix[row_ind, col_ind].sum()\n",
    "\n",
    "def multi_shape(shape_files):\n",
    "    all_shapes = []\n",
    "    for csv_path, label in shape_files:\n",
    "        paths_XYs = read_csv(csv_path)\n",
    "        for path in paths_XYs:\n",
    "            for XY in path:\n",
    "                key_points = normalisation(XY)\n",
    "                all_shapes.append((key_points, label))\n",
    "    return all_shapes\n",
    "\n",
    "def internalangles(points):\n",
    "    angles = []\n",
    "    for i in range(len(points)):\n",
    "        p1 = points[i - 1]\n",
    "        p2 = points[i]\n",
    "        p3 = points[(i + 1) % len(points)]\n",
    "        ba = p1 - p2\n",
    "        bc = p3 - p2\n",
    "        cosinea = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "        angle = np.arccos(np.clip(cosinea, -1.0, 1.0))\n",
    "        angles.append(np.degrees(angle))\n",
    "    return angles\n",
    "\n",
    "def classify(points, reference_shapes):\n",
    "    best_label = None\n",
    "    best_score = np.inf\n",
    "    best_shape = None\n",
    "\n",
    "    for reference, label in reference_shapes:\n",
    "        try:\n",
    "            score = hungarian_matching(points, reference)\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_label = label\n",
    "            best_shape = reference\n",
    "\n",
    "    # Apply angle constraint only for polygons pentagon or larger\n",
    "    if best_label in ['pentagon', 'hexagon', 'heptagon', 'octagon','nonagon','decagon']:\n",
    "        angles = internalangles(points)\n",
    "        max_angle = max(angles) if angles else 0\n",
    "\n",
    "        if max_angle > 170 or len(angles) < 5:  # Consider it a circle if angles are too large or not enough vertices\n",
    "            best_label = 'circle'\n",
    "            best_shape = [shape for shape, label in reference_shapes if label == 'circle'][0]\n",
    "            best_score = 0  # Reset score since we're overriding the classification\n",
    "\n",
    "\n",
    "    if best_label in ['hexagon', 'heptagon', 'octagon','nonagon','decagon'] and best_score > 3:\n",
    "        best_label = 'circle'\n",
    "        best_shape = [shape for shape, label in reference_shapes if label == 'circle'][0]\n",
    "        best_score = 0  # Reset score since we're overriding the classification\n",
    "    return best_label, best_score, best_shape\n",
    "\n",
    "def align_axis(points):\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(points)\n",
    "    return pca.transform(points), pca\n",
    "\n",
    "def align_scale(reference_shape, original_shape, shape_label):\n",
    "    if shape_label in ['rectangle', 'ellipse', 'line', 'triangle']:\n",
    "        aligned_ref_shape, pca_reference = align_axis(reference_shape)\n",
    "        aligned_original, pca_original = align_axis(original_shape)\n",
    "        xmin, ymin = aligned_original.min(axis=0)\n",
    "        xmax, ymax = aligned_original.max(axis=0)\n",
    "        bounding_width = xmax - xmin\n",
    "        bounding_height = ymax - ymin\n",
    "\n",
    "        ref_xmin, ref_ymin = aligned_ref_shape.min(axis=0)\n",
    "        ref_xmax, ref_ymax = aligned_ref_shape.max(axis=0)\n",
    "        ref_width = ref_xmax - ref_xmin\n",
    "        ref_height = ref_ymax - ref_ymin\n",
    "#error handling\n",
    "        if ref_width == 0 or ref_height == 0:\n",
    "            return original_shape, pca_original\n",
    "\n",
    "        scale_x = bounding_width / ref_width\n",
    "        scale_y = bounding_height / ref_height\n",
    "        scaled_shape = aligned_ref_shape * [scale_x, scale_y]\n",
    "        aligned_shape = scaled_shape + [xmin - ref_xmin * scale_x, ymin - ref_ymin * scale_y]\n",
    "        aligned_shape = pca_original.inverse_transform(aligned_shape)\n",
    "        return aligned_shape, pca_original\n",
    "    \n",
    "    else:\n",
    "        xmin, ymin = original_shape.min(axis=0)\n",
    "        xmax, ymax = original_shape.max(axis=0)\n",
    "        bounding_width = xmax - xmin\n",
    "        bounding_height = ymax - ymin\n",
    "        ref_xmin, ref_ymin = reference_shape.min(axis=0)\n",
    "        ref_xmax, ref_ymax = reference_shape.max(axis=0)\n",
    "        ref_width = ref_xmax - ref_xmin\n",
    "        ref_height = ref_ymax - ref_ymin\n",
    "\n",
    "        # error division\n",
    "        if ref_width == 0 or ref_height == 0:\n",
    "            return original_shape\n",
    "\n",
    "        scale_x = bounding_width / ref_width\n",
    "        scale_y = bounding_height / ref_height\n",
    "        scaled_shape = reference_shape * [scale_x, scale_y]\n",
    "        aligned_shape = scaled_shape + [xmin - ref_xmin * scale_x, ymin - ref_ymin * scale_y]\n",
    "        return aligned_shape, None\n",
    "\n",
    "def add_to_contour(contour, min_points=10):\n",
    "    if len(contour) >= min_points:\n",
    "        return contour\n",
    "    points_needed = min_points - len(contour)\n",
    "    extended_contour = []\n",
    "    for i in range(len(contour)):\n",
    "        extended_contour.append(contour[i])\n",
    "        if i < len(contour) - 1:\n",
    "            next_point = contour[i + 1]\n",
    "        else:\n",
    "            next_point = contour[0]\n",
    "        num_new_points = points_needed // len(contour) + (1 if points_needed % len(contour) > i else 0)\n",
    "        for j in range(1, num_new_points + 1):\n",
    "            new_point = contour[i] + (next_point - contour[i]) * (j / (num_new_points + 1))\n",
    "            extended_contour.append(new_point)\n",
    "        points_needed -= num_new_points\n",
    "    return np.array(extended_contour)\n",
    "\n",
    "#define reference shapes\n",
    "shape_files = [\n",
    "    ('/square.csv', 'square'),\n",
    "    ('/star.csv', 'star'),\n",
    "    ('/circle.csv', 'circle'),\n",
    "    ('/ellipse.csv', 'ellipse'),\n",
    "    ('/pentagon.csv', 'pentagon'),\n",
    "    ('/hexagon.csv', 'hexagon'),\n",
    "    ('/heptagon.csv', 'heptagon'),\n",
    "    ('/octagon.csv', 'octagon'),\n",
    "    ('/nonagon.csv', 'nonagon'),\n",
    "    ('/decagon.csv', 'decagon'),\n",
    "    ('/rectangle.csv', 'rectangle'),\n",
    "    ('/triangle.csv', 'triangle'),\n",
    "    ('/straight_line.csv', 'line')\n",
    "]\n",
    "\n",
    "\n",
    "# Define the symmetry types for known shapes\n",
    "symmetry_info = {\n",
    "    'square': '4 lines of reflectional symmetry, 4 rotational symmetries (90° each)',\n",
    "    'nonagon': '9 lines of reflectional symmetry, 9 rotational symmetries (40° each)',\n",
    "    'decagon': '10 lines of reflectional symmetry, 10 rotational symmetries (36° each)',\n",
    "    'star': '5 lines of reflectional symmetry, 5 rotational symmetries (72° each)',\n",
    "    'circle': 'Radial Symmetry',\n",
    "    'pentagon': '5 lines of reflectional symmetry, 5 rotational symmetries (72° each)',\n",
    "    'hexagon': '6 lines of reflectional symmetry, 6 rotational symmetries (60° each)',\n",
    "    'heptagon': '7 lines of reflectional symmetry, 7 rotational symmetries (51.43° each)',\n",
    "    'octagon': '8 lines of reflectional symmetry, 8 rotational symmetries (45° each)',\n",
    "    'rectangle': '2 lines of reflectional symmetry, 2 rotational symmetries (180° each)',\n",
    "    'triangle': '3 lines of reflectional symmetry, 3 rotational symmetries (120° each)',\n",
    "    'line': '1 line of reflectional symmetry, 1 rotational symmetry (0°)',\n",
    "    'ellipse': '2 lines of reflectional symmetry, 2 rotational symmetries (180° each)'\n",
    "    \n",
    "}\n",
    "reference_shapes = multi_shape(shape_files)\n",
    "\n",
    "new_path = input(\"Enter path to shape's CSV file: \")\n",
    "new_paths_XYs = read_csv(new_path)\n",
    "\n",
    "original_shapes = []\n",
    "replaced_shapes = []\n",
    "bounding_boxes = []\n",
    "symmetry_table = []\n",
    "irregular_shapes = []\n",
    "\n",
    "for path in new_paths_XYs:\n",
    "    for XY in path:\n",
    "        if len(XY) < 10:\n",
    "            XY = add_to_contour(XY)  \n",
    "        normalized_points = normalisation(XY)\n",
    "        best_label, best_score, best_shape = classify(normalized_points, reference_shapes)\n",
    "        if best_shape is None:\n",
    "            continue  \n",
    "\n",
    "        if best_score < 10:\n",
    "            symmetry_table.append([best_label, symmetry_info[best_label]])\n",
    "            transformed_shape, pca = align_scale(best_shape, XY, best_label)\n",
    "            bounding_boxes.append((transformed_shape, pca))\n",
    "        else:\n",
    "            transformed_shape = XY\n",
    "            irregular_shapes.append(XY) \n",
    "        original_shapes.append(XY)\n",
    "        replaced_shapes.append(transformed_shape)\n",
    "\n",
    "\n",
    "\n",
    "def symmetry_check(irregular_shapes, extension_factor=3, symmetry_threshold=200):\n",
    "    results = []\n",
    "    for idx, shape in enumerate(irregular_shapes):\n",
    "        if shape.shape[0] < 2:\n",
    "            continue\n",
    "        centroid = np.mean(shape, axis=0)\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(shape)\n",
    "        components = pca.components_\n",
    "\n",
    "        majoraxis = components[0]\n",
    "        projection_major = np.dot(shape - centroid, majoraxis)\n",
    "        left_major = shape[projection_major <= 0]\n",
    "        right_major = shape[projection_major > 0]\n",
    "        right_flipped_major = right_major - 2 * np.outer(projection_major[projection_major > 0], majoraxis)\n",
    "        cost_matrix_major = cdist(left_major, right_flipped_major, metric='euclidean')\n",
    "        row_ind_major, col_ind_major = linear_sum_assignment(cost_matrix_major)\n",
    "        hungarian_major = cost_matrix_major[row_ind_major, col_ind_major].sum()\n",
    "        symmetry_major = 1 if hungarian_major < symmetry_threshold else 0\n",
    "\n",
    "        minoraxis = components[1]\n",
    "        projection_minor = np.dot(shape - centroid, minoraxis)\n",
    "        left_minor = shape[projection_minor <= 0]\n",
    "        right_minor = shape[projection_minor > 0]\n",
    "        right_flipped_minor = right_minor - 2 * np.outer(projection_minor[projection_minor > 0], minoraxis)\n",
    "        cost_matrix_minor = cdist(left_minor, right_flipped_minor, metric='euclidean')\n",
    "        row_ind_minor, col_ind_minor = linear_sum_assignment(cost_matrix_minor)\n",
    "        hungarian_minor = cost_matrix_minor[row_ind_minor, col_ind_minor].sum()\n",
    "        symmetry_minor = 1 if hungarian_minor < symmetry_threshold else 0\n",
    "        results.append([idx + 1, symmetry_major + symmetry_minor])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['Irregular Shape ID', 'Number of Symmetries (LOS)'])\n",
    "    return results_df\n",
    "\n",
    "symmetry_results_df = symmetry_check(irregular_shapes)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for XY in original_shapes:\n",
    "    ax.plot(XY[:, 0], XY[:, 1], 'black', alpha=0.5)\n",
    "\n",
    "plt.title('Original Shape')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for transformed_shape in replaced_shapes:\n",
    "    ax.plot(transformed_shape[:, 0], transformed_shape[:, 1], 'blue', alpha=0.5)\n",
    "\n",
    "plt.title('Regularised Shapes')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for idx, XY in enumerate(irregular_shapes):\n",
    "    ax.plot(XY[:, 0], XY[:, 1], 'red', alpha=0.5)\n",
    "    centroid = np.mean(XY, axis=0)\n",
    "    ax.text(centroid[0], centroid[1], f'ID {idx + 1}', color='red', fontsize=8, ha='center')\n",
    "\n",
    "plt.title('Irregular Shapes that were not changed')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.grid(True)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
